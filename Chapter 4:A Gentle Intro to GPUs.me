# 🔰 Chapter 4: GPU 硬件架构入门
> **⏱️ 视频时间**：01:35:48 - 01:51:40  
> **💡 核心概要**：本章从硬件底层出发，通过对比 CPU 与 GPU 的设计哲学，解释了为什么 GPU 是 AI 时代的基石。同时介绍了 NVIDIA 架构演进史及 CUDA 编程的标准流程。

---

## 1. ⚔️ 硬件架构对比：CPU vs. GPU
这是理解并行计算的起点。两者在设计理念上代表了两种极端的权衡。

### 1.1 核心差异对比表

| 特性 | 🧠 CPU (The Host) | ⚡ GPU (The Device) |
| :--- | :--- | :--- |
| **设计目标** | **低延迟 (Low Latency)** | **高吞吐量 (High Throughput)** |
| **擅长任务** | 复杂的逻辑控制、串行指令流 | 大规模数据并行计算、图形渲染 |
| **核心数量** | 少 (Few Cores, 几十个) | 多 (Many Cores, 成千上万) |
| **单核主频** | 高 (High Clock Speed) | 低 (Low Clock Speed) |
| **缓存 (Cache)** | **大** (占据芯片主要面积) | **小** (大部分面积留给计算单元 ALU) |
| **比喻** | 🏎️ **法拉利跑车**<br>运送少量货物极快。 | 🚌 **满载的大巴/火车**<br>单次慢，但一次运几千人，总量完胜。 |

### 1.2 其他硬件形态
*   **TPU (Tensor Processing Unit)**：Google 定制的 ASIC。砍掉了图形渲染功能，专为矩阵乘法极致优化。
*   **FPGA (现场可编程门阵列)**：硬件电路可重构。能实现极低延迟，但开发成本高、难度大。

---

## 2. 🧩 为什么 GPU 适合深度学习？

### 2.1 拼图游戏类比 (The Jigsaw Puzzle Analogy)
> 想象你要完成一个 10,000 片的拼图：
> *   **CPU 模式**：4 个手速极快的专家（4核）。每个人拼得很快，但人太少，整体慢。
> *   **GPU 模式**：4,000 个普通人（4000核）。虽然每个人动作稍慢，但人海战术瞬间完成。

### 2.2 深度学习的物理本质
深度学习的核心计算是 **矩阵乘法 (GEMM)**，这是一种典型的 **数据并行 (Data Parallel)** 任务：
*   计算矩阵中一个元素的值，不依赖于其他元素。
*   这种“互不干扰”的特性，完美契合 GPU 的众核架构。

---

## 3. 📜 NVIDIA GPU 架构演进史
了解架构代号（Code Name）对查阅文档和优化至关重要。

*   **🕹️ 早期 (1993-2006)**：`GeForce` 系列。仅用于图形渲染，固定管线。
*   **🌊 CUDA 时代开启**：
    *   **Tesla (G80)**：2006年，GPU 走向通用计算 (GPGPU) 的起点。
    *   **Fermi**：引入 L1/L2 Cache，完善双精度计算。
*   **🧠 深度学习爆发期**：
    *   **Pascal (P100)**：引入 HBM2 显存，带宽大幅提升。
    *   **Volta (V100)**：🚩 **里程碑**。引入 **Tensor Core**，专门加速 $4 \times 4$ 矩阵运算。
    *   **Ampere (A100/RTX 30)**：支持稀疏性 (Sparsity) 和 TF32 格式。
    *   **Hopper (H100)**：专为 Transformer 优化，支持 FP8。
    *   **Blackwell**：最新架构，互联带宽再次突破。

> **💡 提示**：使用 `nvidia-smi` 查看显卡型号，并查阅其 **Compute Capability**（计算能力），这决定了你能使用哪些 CUDA 高级特性。

---

## 4. 🔄 典型的 CUDA 程序流程
一个标准的 CUDA 程序严格遵循 **Host-Device** 模型，数据必须在“两端”搬运。

1.  **Allocate (分配)** 🛠️
    *   CPU 在 Host 内存中分配空间。
2.  **Copy H2D (搬运)** 🚚
    *   CPU 将数据从 Host 内存复制到 GPU 显存 (Device Memory)。
    *   ⚠️ **瓶颈预警**：这是主要性能瓶颈之一 (受限于 PCIe 带宽)。
3.  **Launch Kernel (启动)** 🚀
    *   CPU 指挥 GPU 启动 Kernel。
    *   计算完全在 GPU 上并行执行。
    *   CPU 可异步处理其他任务。
4.  **Copy D2H (回传)** 📦
    *   计算完成，结果从 GPU 显存拷回 Host 内存。
5.  **Use (使用)** 📊
    *   CPU 使用结果进行后续逻辑或展示。

---

## 5. 📖 核心术语清单 (Vocabulary)
进入实战代码前，必须掌握的“黑话”：

*   **💻 Host (主机)**
    *   指 **CPU** 及其系统内存。执行普通 C++ 代码。
*   **🔋 Device (设备)**
    *   指 **GPU** 及其显存 (VRAM)。执行 CUDA Kernel。
*   **🌽 Kernel (内核)**
    *   不是爆米花，也不是操作系统内核。
    *   它是 **运行在 GPU 上的函数**。
    *   代码中用 `__global__` 标记。
    *   **写法像串行，执行是并行**（被成千上万个线程同时调用）。
*   **🧮 GEMM**
    *   **Ge**neral **M**atrix **M**ultiplication（通用矩阵乘法）。AI 算力的原子单位。
    *   **SGEMM**：**S**ingle Precision (**FP32**) GEMM（单精度矩阵乘法）。
*   **🏗️ Hierarchy (层级)**
    *   下一章重点：**Grid (网格) -> Block (块) -> Thread (线程)**。
    *   这是 CUDA 组织成千上万个核心的逻辑结构。

---
> **🧠 深度总结**
> 这一章是思维模式的转折点。从 CPU 到 GPU，必须从“如何让这一步更快”转变为 **“如何让一万步同时发生”**。
> 这里的关键在于理解 **Memory Bandwidth**（内存带宽）：因为 GPU 计算太快了，绝大多数时间它其实是在“等数据”（IO Bound）。优化 CUDA 的本质，往往就是优化数据的搬运。
