视频时间 00:00-16:54
这一部分是整个课程的基石，不仅介绍了“我们要学什么”，更深刻地阐述了“为什么现在学习 GPU 编程具有极高的价值”。
学习建议与预备知识 (Prerequisites)
必须具备: Python 基础，对深度学习的基本概念（神经网络、矩阵乘法）有直观理解。
建议具备: C/C++ 基础。如果完全没有 C 语言（指针、内存管理）的概念，会比较吃力。
心态: 这是一个“陡峭”的学习曲线。不仅仅是学习语法，更是在学习计算机体系结构（Computer Architecture）。
1. 核心论题：为什么你需要学习 CUDA？(The Motivation)
1.1 跨越“护城河” (The Moat)
在当前的 AI 浪潮中，会使用 PyTorch 搭建模型的人很多，但能够深入底层、编写高性能 GPU 内核（Kernel）的人极少。
稀缺性：能够理解硬件如何工作，并编写出比默认 PyTorch 实现快 2 倍、10 倍甚至 100 倍代码的工程师，在就业市场上拥有巨大的竞争优势。
工程本质：深度学习工程（ML Engineering）在很多时候不仅仅是关于模型架构的设计，更多的是关于数据移动（Data Movement）。
核心技能：CUDA 编程不仅仅是写 C++ 代码，它是一种思维方式——如何将数据搬运到 GPU 芯片上，进行并行计算，然后再搬运出去。掌握这种思维方式是你职业生涯的“护城河”。
1.2 摩尔定律的终结与黄氏定律的崛起 (Moore’s Law vs. Huang’s Law)
CPU 的瓶颈：传统的 CPU 单核性能提升已经趋于平缓（摩尔定律在单核性能上已不再显著生效）。我们不能再指望仅仅通过等待下一代 Intel/AMD CPU 来获得代码的自动加速。
GPU 的爆发：NVIDIA CEO 黄仁勋提出的“黄氏定律”（Huang’s Law）指出，GPU 的计算性能在过去十年中呈现出超越摩尔定律的指数级增长。
范式转移：为了利用这种算力增长，程序员必须改变思维模式——从串行编程（Sequential Programming）转向大规模并行编程（Massively Parallel Programming）。
2. 深度学习的物理瓶颈：内存墙 (The Memory Wall)
这是本节最关键的技术概念。理解“内存墙”是进行所有 CUDA 优化的前提。
2.1 算力 vs. 带宽
视频中对比了两个核心指标的增长速度：
Compute (FLOPS)：GPU 进行数学运算（加法、乘法）的能力。
Memory Bandwidth (GB/s)：将数据从显存（HBM/VRAM）搬运到计算单元的能力。
2.2 瓶颈的形成
不匹配的增长：在过去 20 年里，GPU 的计算能力提升了数万倍，但内存带宽仅提升了数百倍。两者之间的差距越来越大。
吸管与杯子 (The Straw and The Cup)：  
想象你是一个极度口渴的人（计算单元），你想喝光杯子里的水（数据）。
你的喝水速度极快（计算能力强），但你只有一根非常细的吸管（内存带宽）。
结果：无论你喝水的能力有多强，你摄入水的速度最终取决于吸管的粗细。
结论：目前的深度学习模型（尤其是 LLM）主要是 Memory Bound（受限于内存带宽），而不是 Compute Bound（受限于计算能力）。
优化的本质：CUDA 优化的核心往往不是“如何算得更快”，而是“如何更聪明地移动数据”，以减少对那根细吸管的依赖。
3. AI 软件栈全景 (The AI Ecosystem Stack)
我们需要知道自己在整个技术栈中的位置。
3.1 顶层：用户与框架 (User & Frameworks)
Tools: PyTorch, TensorFlow, JAX.
特点: 易用性高，开发速度快，使用 Python。
局限: 当需要极致性能或实现非标准算子时，纯 Python 无法满足需求。
3.2 中间层：编译器与运行时 (Compilers & Runtime)
Tools: ONNX Runtime, TensorRT.
作用: 将上层的 Python 代码转换为更高效的机器表示，进行图层面的优化（Graph Optimization），如算子融合（Operator Fusion）。
3.3 核心层：内核语言 (Kernel Languages) —— 本课程关注点
Tools: CUDA, Triton.
定位: 这是连接高层算法与底层晶体管的桥梁。在这里，你需要手动管理内存、线程和同步。  
CUDA: NVIDIA 的原生语言，控制力最强，难度最高。
Triton: OpenAI 推出的类 Python 语言，旨在简化 GPU 编程，同时保持高性能。
3.4 底层：硬件 (Hardware)
Components: Transistors, Gates, PTX (Parallel Thread Execution), SASS (Assembly).
作用: 执行最终的指令。
4. 课程路线图 (Course Curriculum)
Phase 1: 基础 (Foundations)
C/C++ Refresher: 快速回顾指针、内存分配、编译流程（Makefiles）。
GPU Architecture: 理解 SM（Streaming Multiprocessor）、VRAM、L1/L2 Cache。
First Kernels: 编写向量加法（Vector Add），理解 Grid、Block、Thread 的层级结构。
Phase 2: 性能与工具 (Profiling & Tools)
Nsight Compute: 学会使用专业工具来“看”代码的运行情况。如果你不能测量它，你就不能优化它。
Roofline Model: 一个评估性能的理论模型，帮助你判断代码是受限于带宽还是算力。
Phase 3: 核心算法与优化 (Algorithms & Optimization)
Matrix Multiplication (GEMM): 深度学习的原子操作。课程将花大量时间，从朴素实现一步步优化到接近 cuBLAS（NVIDIA 官方库）的性能。
Optimization Techniques:  
Coalescing: 内存合并访问。
Shared Memory Tiling: 利用共享内存进行分块。
Vectorization: 向量化读写。
Phase 4: 进阶与实战 (Advanced & Practice)
Triton: 学习下一代 GPU 编程语言。
Final Project: 从零构建 MNIST 训练流程。  
不依赖 PyTorch 的自动求导（Autograd）。
自己写 Python 逻辑 -> 自己写 C 扩展 -> 自己写 CUDA Kernel。
实现前向传播、反向传播、权重更新、激活函数。
